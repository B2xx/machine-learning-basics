{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4psGWYPxLgRt"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "# from numba import jit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yvCR81frLgRx"
      },
      "outputs": [],
      "source": [
        "#TODO: loss of least square regression and binary logistic regression\n",
        "'''\n",
        "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
        "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
        "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
        "'''\n",
        "class leastsquare(object):\n",
        "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
        "    def pred(self,score):\n",
        "        return score\n",
        "\n",
        "    def g(self,true,score):\n",
        "      return 2 * (score - true)\n",
        "      pass\n",
        "\n",
        "    def h(self,true,score):\n",
        "      return 2 * np.ones_like(score)\n",
        "      pass\n",
        "\n",
        "class logistic(object):\n",
        "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
        "    def pred(self,score):\n",
        "      return 1 / (1 + np.exp(-score))\n",
        "      pass\n",
        "\n",
        "    def g(self,true,score):\n",
        "      return self.pred(score) - true\n",
        "      pass\n",
        "\n",
        "    def h(self,true,score):\n",
        "      p = self.pred(score)\n",
        "      return p * (1 - p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Lv8-f4iLgRy"
      },
      "outputs": [],
      "source": [
        "# TODO: class of Random Forest\n",
        "\n",
        "# Import mode function\n",
        "from scipy.stats import mode\n",
        "\n",
        "class RF(object):\n",
        "    '''\n",
        "    Class of Random Forest\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        loss: Loss function for gradient boosting.\n",
        "            'mse' for regression task and 'log' for classfication task.\n",
        "            A child class of the loss class could be passed to implement customized loss.\n",
        "        max_depth: The maximum depth d_max of a tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
        "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
        "        num_trees: Number of trees.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "        n_threads = None, loss = 'mse',\n",
        "        max_depth = 3, min_sample_split = 10,\n",
        "        lamda = 1, gamma = 0,\n",
        "        rf = 0.99, num_trees = 100):\n",
        "\n",
        "        self.n_threads = n_threads\n",
        "        self.loss = loss\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.rf = rf\n",
        "        self.num_trees = num_trees\n",
        "\n",
        "    def fit(self, train, target):\n",
        "        # train is n x m 2d numpy array\n",
        "        # target is n-dim 1d array\n",
        "        #TODO\n",
        "        self.trees = []\n",
        "        n_samples, n_features = train.shape\n",
        "        for _ in range(self.num_trees):\n",
        "            # Bootstrap sample\n",
        "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            train_sample = train[indices]\n",
        "            target_sample = target[indices]\n",
        "            # Train a tree\n",
        "            tree = Tree(\n",
        "                n_threads=self.n_threads,\n",
        "                max_depth=self.max_depth,\n",
        "                min_sample_split=self.min_sample_split,\n",
        "                lamda=self.lamda,\n",
        "                gamma=self.gamma,\n",
        "                rf=self.rf,\n",
        "                loss=self.loss  # Pass the loss function\n",
        "            )\n",
        "            tree.fit(train_sample, target_sample)\n",
        "            self.trees.append(tree)\n",
        "        return self\n",
        "\n",
        "    def predict(self, test):\n",
        "        # Aggregate predictions from all trees\n",
        "        predictions = np.array([tree.predict(test) for tree in self.trees])\n",
        "        if self.loss == 'log':\n",
        "            # Majority voting for classification\n",
        "            pred_labels, _ = mode(predictions, axis=0)\n",
        "            return pred_labels[0]\n",
        "        else:\n",
        "            # Average for regression\n",
        "            return np.mean(predictions, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NDBSQF4QLgRz"
      },
      "outputs": [],
      "source": [
        "# TODO: class of GBDT\n",
        "class GBDT(object):\n",
        "    '''\n",
        "    Class of gradient boosting decision tree (GBDT)\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        loss: Loss function for gradient boosting.\n",
        "            'mse' for regression task and 'log' for classfication task.\n",
        "            A child class of the loss class could be passed to implement customized loss.\n",
        "        max_depth: The maximum depth D_max of a tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
        "        learning_rate: The learning rate eta of GBDT.\n",
        "        num_trees: Number of trees.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "        n_threads = None, loss = 'mse',\n",
        "        max_depth = 3, min_sample_split = 10,\n",
        "        lamda = 1, gamma = 0,\n",
        "        learning_rate = 0.1, num_trees = 100):\n",
        "\n",
        "        self.n_threads = n_threads\n",
        "        if loss == 'mse':\n",
        "            self.loss = leastsquare()\n",
        "        elif loss == 'log':\n",
        "            self.loss = logistic()\n",
        "        else:\n",
        "            self.loss = loss  # Assume it's a loss object\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_trees = num_trees\n",
        "        self.loss_name = loss\n",
        "\n",
        "    def fit(self, train, target):\n",
        "        # train is n x m 2d numpy array\n",
        "        # target is n-dim 1d array\n",
        "        #TODO\n",
        "        self.trees = []\n",
        "        n_samples = train.shape[0]\n",
        "        # Initialize predictions to zero\n",
        "        self.score = np.zeros(n_samples)\n",
        "        for _ in range(self.num_trees):\n",
        "            # Compute gradients and hessians\n",
        "            g = self.loss.g(target, self.score)\n",
        "            h = self.loss.h(target, self.score)\n",
        "            # Fit a tree to the negative gradients\n",
        "            tree = Tree(\n",
        "                n_threads=self.n_threads,\n",
        "                max_depth=self.max_depth,\n",
        "                min_sample_split=self.min_sample_split,\n",
        "                lamda=self.lamda,\n",
        "                gamma=self.gamma,\n",
        "                loss=self.loss_name\n",
        "            )\n",
        "            tree.fit(train, None, g, h)\n",
        "            # Update the scores\n",
        "            pred = tree.predict(train)\n",
        "            self.score += self.learning_rate * pred\n",
        "            self.trees.append(tree)\n",
        "        return self\n",
        "\n",
        "    def predict(self, test):\n",
        "        #TODO\n",
        "        # Initialize predictions to zero\n",
        "        score = np.zeros(test.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred = tree.predict(test)\n",
        "            score += self.learning_rate * pred\n",
        "        if self.loss_name == logistic():\n",
        "          return (self.loss.pred(score)>0.5).astype(int)\n",
        "        else:\n",
        "          return self.loss.pred(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ir5NMnXcLgR0"
      },
      "outputs": [],
      "source": [
        "# TODO: class of a node on a tree\n",
        "class TreeNode(object):\n",
        "    '''\n",
        "    Data structure that are used for storing a node on a tree.\n",
        "\n",
        "    A tree is presented by a set of nested TreeNodes,\n",
        "    with one TreeNode pointing two child TreeNodes,\n",
        "    until a tree leaf is reached.\n",
        "\n",
        "    A node on a tree can be either a leaf node or a non-leaf node.\n",
        "    '''\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, is_leaf=False, prediction=None, feature_index=None, threshold=None,\n",
        "                 left_child=None, right_child=None):\n",
        "        self.is_leaf = is_leaf\n",
        "        self.prediction = prediction  # Value to predict if leaf node\n",
        "        self.feature_index = feature_index  # Index of feature to split on\n",
        "        self.threshold = threshold  # Threshold value to split at\n",
        "        self.left_child = left_child  # Left child node\n",
        "        self.right_child = right_child  # Right child node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LCpHBigMLgR0"
      },
      "outputs": [],
      "source": [
        "# TODO: class of single tree\n",
        "class Tree(object):\n",
        "    '''\n",
        "    Class of a single decision tree in GBDT or RF\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        max_depth: The maximum depth of the tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
        "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
        "            rf = 0 means we are training a GBDT.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_threads = None,\n",
        "                 max_depth = 3, min_sample_split = 10,\n",
        "                 lamda = 1, gamma = 0, rf = 0, loss = 'mse'):\n",
        "        self.n_threads = n_threads\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.rf = rf\n",
        "        self.int_member = 0\n",
        "        self.loss = loss\n",
        "\n",
        "        self.feature_indices = None  # To store the subset of features used\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, train, target, g=None, h=None):\n",
        "        '''\n",
        "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
        "        g and h are gradient and hessian respectively.\n",
        "        '''\n",
        "        #TODO\n",
        "        if g is not None and h is not None:\n",
        "            self.is_gbdt = True\n",
        "            # print(\"GBDT\",self.is_gbdt)\n",
        "        else:\n",
        "            self.is_gbdt = False\n",
        "            #print(\"GBDT\",self.is_gbdt)\n",
        "        if not self.is_gbdt and self.loss == 'log':\n",
        "            self.is_classification = True\n",
        "            #print(\"RF and classification\",self.is_classification)\n",
        "        else:\n",
        "            self.is_classification = False\n",
        "            # print(\"RF and classification\",self.is_classification)\n",
        "        n_samples, n_features = train.shape\n",
        "        if self.rf > 0:\n",
        "            m = n_features\n",
        "            k = max(1, int(self.rf * m))\n",
        "            self.feature_indices = np.random.choice(m, k, replace=False)\n",
        "        else:\n",
        "            self.feature_indices = np.arange(n_features)\n",
        "        self.root = self.construct_tree(train, target, g, h, depth=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self,test):\n",
        "        '''\n",
        "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
        "        Return predictions (scores) as an array.\n",
        "        '''\n",
        "        #TODO\n",
        "        result = np.array([self._predict_row(x, self.root) for x in test])\n",
        "        return result\n",
        "\n",
        "    def _predict_row(self, x, node):\n",
        "      if node.is_leaf:\n",
        "          return node.prediction\n",
        "      else:\n",
        "          if x[node.feature_index] <= node.threshold:\n",
        "              return self._predict_row(x, node.left_child)\n",
        "          else:\n",
        "              return self._predict_row(x, node.right_child)\n",
        "\n",
        "    def construct_tree(self, train, target, g, h, depth):\n",
        "        '''\n",
        "        Tree construction, which is recursively used to grow a tree.\n",
        "        First we should check if we should stop further splitting.\n",
        "\n",
        "        The stopping conditions include:\n",
        "            1. tree reaches max_depth $d_{max}$\n",
        "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
        "            3. gain <= 0\n",
        "        '''\n",
        "        #TODO\n",
        "        n_samples, n_features = train.shape\n",
        "        if depth >= self.max_depth or n_samples <= self.min_sample_split:\n",
        "            # Make a leaf node\n",
        "            if self.is_gbdt:\n",
        "                G = np.sum(g)\n",
        "                H = np.sum(h)\n",
        "                pred = -G / (H + self.lamda)\n",
        "            elif self.is_classification:\n",
        "                # Predict the majority class\n",
        "                counts = np.bincount(target)\n",
        "                pred = np.argmax(counts)\n",
        "            else:\n",
        "                pred = np.mean(target)\n",
        "            return TreeNode(is_leaf=True, prediction=pred)\n",
        "        else:\n",
        "            # Find the best decision rule\n",
        "            feature, threshold, gain = self.find_best_decision_rule(train, target, g, h)\n",
        "            if gain <= 0 or feature is None:\n",
        "                # Make a leaf node\n",
        "                if self.is_gbdt:\n",
        "                    G = np.sum(g)\n",
        "                    H = np.sum(h)\n",
        "                    pred = -G / (H + self.lamda)\n",
        "                elif self.is_classification:\n",
        "                    # Predict the majority class\n",
        "                    counts = np.bincount(target)\n",
        "                    pred = np.argmax(counts)\n",
        "                else:\n",
        "                    pred = np.mean(target)\n",
        "                return TreeNode(is_leaf=True, prediction=pred)\n",
        "            else:\n",
        "                # Split the data\n",
        "                idx_left = train[:, feature] <= threshold\n",
        "                idx_right = train[:, feature] > threshold\n",
        "                if self.is_gbdt:\n",
        "                    left_child = self.construct_tree(train[idx_left], None, g[idx_left], h[idx_left], depth + 1)\n",
        "                    right_child = self.construct_tree(train[idx_right], None, g[idx_right], h[idx_right], depth + 1)\n",
        "                else:\n",
        "                    left_child = self.construct_tree(train[idx_left], target[idx_left], None, None, depth + 1)\n",
        "                    right_child = self.construct_tree(train[idx_right], target[idx_right], None, None, depth + 1)\n",
        "                return TreeNode(\n",
        "                    is_leaf=False,\n",
        "                    feature_index=feature,\n",
        "                    threshold=threshold,\n",
        "                    left_child=left_child,\n",
        "                    right_child=right_child\n",
        "                )\n",
        "\n",
        "        # return TreeNode(split_feature = feature, split_threshold = threshold,\n",
        "        #                 left_child = left_child, right_child = right_child)\n",
        "\n",
        "    def find_best_decision_rule(self, train, target, g, h):\n",
        "        '''\n",
        "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j,\n",
        "        train is the training data assigned to node j\n",
        "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
        "        g and h should be vectors of the same length as the number of data points in train\n",
        "\n",
        "        for each feature, we find the best threshold by find_threshold(),\n",
        "        a [threshold, best_gain] list is returned for each feature.\n",
        "        Then we select the feature with the largest best_gain,\n",
        "        and return the best decision rule [feature, treshold] together with its gain.\n",
        "        '''\n",
        "        #TODO\n",
        "        n_samples, n_features = train.shape\n",
        "        # Prepare data for multiprocessing\n",
        "        is_gbdt = self.is_gbdt\n",
        "        is_classification = self.is_classification\n",
        "        lamda = self.lamda\n",
        "        gamma = self.gamma\n",
        "\n",
        "        # Prepare arguments for parallel processing\n",
        "        args_list = []\n",
        "        for feature in self.feature_indices:\n",
        "            feature_values = train[:, feature]\n",
        "            args = (feature, feature_values, target, g, h, is_gbdt, is_classification, lamda, gamma)\n",
        "            args_list.append(args)\n",
        "\n",
        "        # Determine the number of processes\n",
        "        if self.n_threads is None:\n",
        "            num_processes = multiprocessing.cpu_count()\n",
        "        else:\n",
        "            num_processes = self.n_threads\n",
        "\n",
        "        # Use multiprocessing to find the best thresholds for each feature in parallel\n",
        "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(find_best_for_feature, args_list)\n",
        "\n",
        "        # Select the best feature and threshold based on gain\n",
        "        best_gain = -np.inf\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        for feature, threshold, gain in results:\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "# Define the function for parallel computation\n",
        "def find_best_for_feature(args):\n",
        "    feature, feature_values, target, g, h, is_gbdt, is_classification, lamda, gamma = args\n",
        "    threshold, gain = find_threshold_static(feature_values, target, g, h, is_gbdt, is_classification, lamda, gamma)\n",
        "    return feature, threshold, gain\n",
        "\n",
        "# Define the static method for finding the best threshold\n",
        "def find_threshold_static(feature_values, target, g, h, is_gbdt, is_classification, lamda, gamma):\n",
        "    sorted_idx = np.argsort(feature_values)\n",
        "    feature_values = feature_values[sorted_idx]\n",
        "    if is_gbdt:\n",
        "        g = g[sorted_idx]\n",
        "        h = h[sorted_idx]\n",
        "        G_total = np.sum(g)\n",
        "        H_total = np.sum(h)\n",
        "        G_left = 0.0\n",
        "        H_left = 0.0\n",
        "    elif is_classification:\n",
        "        target = target[sorted_idx]\n",
        "        n_total = len(target)\n",
        "        # Compute Gini impurity of the parent node\n",
        "        classes, counts = np.unique(target, return_counts=True)\n",
        "        p_classes = counts / n_total\n",
        "        gini_parent = 1.0 - np.sum(p_classes ** 2)\n",
        "    else:\n",
        "        target = target[sorted_idx]\n",
        "        total_variance = np.var(target) * len(target)\n",
        "        sum_left = 0.0\n",
        "        sum_right = np.sum(target)\n",
        "        sum_sq_left = 0.0\n",
        "        sum_sq_right = np.sum(target ** 2)\n",
        "        n_left = 0\n",
        "        n_right = len(target)\n",
        "    best_gain = -np.inf\n",
        "    best_threshold = None\n",
        "    for i in range(1, len(feature_values)):\n",
        "        if feature_values[i] == feature_values[i - 1]:\n",
        "            continue  # Skip if the threshold is the same\n",
        "        if is_gbdt:\n",
        "            G_left += g[i - 1]\n",
        "            H_left += h[i - 1]\n",
        "            G_right = G_total - G_left\n",
        "            H_right = H_total - H_left\n",
        "            # Compute gain\n",
        "            gain = 0.5 * (\n",
        "                G_left ** 2 / (H_left + lamda) +\n",
        "                G_right ** 2 / (H_right + lamda) -\n",
        "                G_total ** 2 / (H_total + lamda)\n",
        "            ) - gamma\n",
        "        elif is_classification:\n",
        "            # Update left and right class counts\n",
        "            left_classes, left_counts = np.unique(target[:i], return_counts=True)\n",
        "            right_classes, right_counts = np.unique(target[i:], return_counts=True)\n",
        "            n_left = i\n",
        "            n_right = len(target) - i\n",
        "            gini_left = 1.0 - np.sum((left_counts / n_left) ** 2)\n",
        "            gini_right = 1.0 - np.sum((right_counts / n_right) ** 2)\n",
        "            # Compute weighted Gini impurity\n",
        "            gini_split = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
        "            # Compute gain\n",
        "            gain = gini_parent - gini_split\n",
        "        else:\n",
        "            # Update left and right sums\n",
        "            val = target[i - 1]\n",
        "            sum_left += val\n",
        "            sum_right -= val\n",
        "            sum_sq_left += val ** 2\n",
        "            sum_sq_right -= val ** 2\n",
        "            n_left += 1\n",
        "            n_right -= 1\n",
        "            # Compute variance reduction\n",
        "            if n_left > 0 and n_right > 0:\n",
        "                var_left = sum_sq_left - (sum_left ** 2) / n_left\n",
        "                var_right = sum_sq_right - (sum_right ** 2) / n_right\n",
        "                gain = total_variance - (var_left + var_right)\n",
        "            else:\n",
        "                gain = 0\n",
        "        if gain > best_gain:\n",
        "            best_gain = gain\n",
        "            best_threshold = (feature_values[i] + feature_values[i - 1]) / 2\n",
        "    return best_threshold, best_gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B6Ofw3okLgR1"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
        "\n",
        "# RMSE\n",
        "def root_mean_square_error(pred, y):\n",
        "  #TODO\n",
        "  rmse = np.sqrt(np.mean((pred - y) ** 2))\n",
        "  return rmse\n",
        "\n",
        "# precision\n",
        "def accuracy(pred, y):\n",
        "  #TODO\n",
        "  pred_labels = np.round(pred)\n",
        "  return np.mean(pred_labels == y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCsb5rMoLgR1",
        "outputId": "a16bd26a-0cc6-4939-c8ba-791f97dd4f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n",
            "RF Test RMSE: 5.286105440163269\n",
            "RF Training RMSE: 4.648074557108212\n",
            "GBDT Test RMSE: 4.554350406941252\n",
            "GBDT Training RMSE: 3.50478513939429\n"
          ]
        }
      ],
      "source": [
        "# TODO: GBDT regression on boston house price dataset\n",
        "\n",
        "# load data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y = raw_df.values[1::2, 2]\n",
        "\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
        "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RF(n_threads = None, loss = 'mse',\n",
        "        max_depth = 6, min_sample_split = 25,\n",
        "        lamda = 5, gamma = 0.5,\n",
        "        rf = 0.35, num_trees = 25)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on Test Data\n",
        "y_pred_rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate RMSE on Test Data\n",
        "rmse_rf_test = root_mean_square_error(y_pred_rf_test, y_test)\n",
        "print(f\"RF Test RMSE: {rmse_rf_test}\")\n",
        "\n",
        "# Make predictions on Training Data\n",
        "y_pred_rf_train = rf_model.predict(X_train)\n",
        "\n",
        "# Evaluate RMSE on Training Data\n",
        "rmse_rf_train = root_mean_square_error(y_pred_rf_train, y_train)\n",
        "print(f\"RF Training RMSE: {rmse_rf_train}\")\n",
        "\n",
        "# GBDT regression on Boston house price dataset\n",
        "\n",
        "# Train GBDT model\n",
        "gbdt_model = GBDT(\n",
        "    n_threads=None,\n",
        "    loss='mse',\n",
        "    max_depth=6,\n",
        "    min_sample_split=25,\n",
        "    lamda=5,\n",
        "    gamma=0.5,\n",
        "    learning_rate=0.1,\n",
        "    num_trees=25\n",
        ")\n",
        "\n",
        "gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Test Data\n",
        "y_pred_gbdt_test = gbdt_model.predict(X_test)\n",
        "rmse_gbdt_test = root_mean_square_error(y_pred_gbdt_test, y_test)\n",
        "print(f\"GBDT Test RMSE: {rmse_gbdt_test}\")\n",
        "\n",
        "# Evaluate on Training Data\n",
        "y_pred_gbdt_train = gbdt_model.predict(X_train)\n",
        "rmse_gbdt_train = root_mean_square_error(y_pred_gbdt_train, y_train)\n",
        "print(f\"GBDT Training RMSE: {rmse_gbdt_train}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU397FXoLgR1",
        "outputId": "460a4867-364b-46ea-9397-ea89e52ae1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 61) (1000,) (700, 61) (700,) (300, 61) (300,)\n",
            "GBDT Test Accuracy on credit-g dataset: 0.6833333333333333\n",
            "GBDT Training Accuracy: 0.7414285714285714\n",
            "RF Test Accuracy: 0.7\n",
            "RF Training Accuracy: 0.7\n"
          ]
        }
      ],
      "source": [
        "# TODO: GBDT classification on credit-g dataset\n",
        "\n",
        "# load data\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
        "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
        "\n",
        "X = pd.get_dummies(X)\n",
        "X = X.values\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
        "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# Train GBDT model\n",
        "gbdt_model = GBDT(\n",
        "n_threads = None, loss = logistic(),\n",
        "        max_depth = 6, min_sample_split = 25,\n",
        "        lamda = 5, gamma = 0.5,\n",
        "        learning_rate = 0.55, num_trees = 25\n",
        ")\n",
        "\n",
        "gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on Test Data\n",
        "y_pred_gbdt_test = gbdt_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on Test Data\n",
        "acc_gbdt_test = accuracy(y_pred_gbdt_test, y_test)\n",
        "print(f\"GBDT Test Accuracy on credit-g dataset: {acc_gbdt_test}\")\n",
        "\n",
        "# Make predictions on Training Data\n",
        "y_pred_gbdt_train = gbdt_model.predict(X_train)\n",
        "\n",
        "# Evaluate accuracy on Training Data\n",
        "acc_gbdt_train = accuracy(y_pred_gbdt_train, y_train)\n",
        "print(f\"GBDT Training Accuracy: {acc_gbdt_train}\")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RF(\n",
        "    n_threads=None,\n",
        "    loss='log',  # Use 'log' loss for classification\n",
        "    max_depth=6,\n",
        "    min_sample_split=25,\n",
        "    lamda=5,\n",
        "    gamma=0.5,\n",
        "    rf=0.35,\n",
        "    num_trees=50\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on Test Data\n",
        "y_pred_rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on Test Data\n",
        "acc_rf_test = accuracy(y_pred_rf_test, y_test)\n",
        "print(f\"RF Test Accuracy: {acc_rf_test}\")\n",
        "\n",
        "# Make predictions on Training Data\n",
        "y_pred_rf_train = rf_model.predict(X_train)\n",
        "\n",
        "# Evaluate accuracy on Training Data\n",
        "acc_rf_train = accuracy(y_pred_rf_train, y_train)\n",
        "print(f\"RF Training Accuracy: {acc_rf_train}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba2-WNqtLgR2",
        "outputId": "38319d5f-b81c-44dc-f744-645fd42f9363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n",
            "GBDT Test Accuracy on breast-cancer dataset: 0.9590643274853801\n",
            "GBDT Training Accuracy: 0.9949748743718593\n",
            "RF Test Accuracy: 0.6140350877192983\n",
            "RF Training Accuracy: 0.6331658291457286\n"
          ]
        }
      ],
      "source": [
        "# TODO: GBDT classification on breast cancer dataset\n",
        "\n",
        "# load data\n",
        "from sklearn import datasets\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
        "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# Train GBDT model\n",
        "gbdt_model = GBDT(\n",
        "    n_threads=None,\n",
        "    loss='log',  # Use logistic loss for classification\n",
        "    max_depth=3,\n",
        "    min_sample_split=25,\n",
        "    lamda=5,\n",
        "    gamma=0.5,\n",
        "    learning_rate=0.55,\n",
        "    num_trees=25\n",
        ")\n",
        "\n",
        "gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on Test Data\n",
        "y_pred_gbdt_test = gbdt_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on Test Data\n",
        "acc_gbdt_test = accuracy(y_pred_gbdt_test, y_test)\n",
        "print(f\"GBDT Test Accuracy on breast-cancer dataset: {acc_gbdt_test}\")\n",
        "\n",
        "# Make predictions on Training Data\n",
        "y_pred_gbdt_train = gbdt_model.predict(X_train)\n",
        "\n",
        "# Evaluate accuracy on Training Data\n",
        "acc_gbdt_train = accuracy(y_pred_gbdt_train, y_train)\n",
        "print(f\"GBDT Training Accuracy: {acc_gbdt_train}\")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RF(\n",
        "    n_threads=None,\n",
        "    loss='log',  # Use 'log' loss for classification\n",
        "    max_depth=6,\n",
        "    min_sample_split=25,\n",
        "    lamda=5,\n",
        "    gamma=0.5,\n",
        "    rf=0.35,\n",
        "    num_trees=50\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on Test Data\n",
        "y_pred_rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on Test Data\n",
        "acc_rf_test = accuracy(y_pred_rf_test, y_test)\n",
        "print(f\"RF Test Accuracy: {acc_rf_test}\")\n",
        "\n",
        "# Make predictions on Training Data\n",
        "y_pred_rf_train = rf_model.predict(X_train)\n",
        "\n",
        "# Evaluate accuracy on Training Data\n",
        "acc_rf_train = accuracy(y_pred_rf_train, y_train)\n",
        "print(f\"RF Training Accuracy: {acc_rf_train}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}